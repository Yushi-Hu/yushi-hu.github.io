---
 layout: archive
---

 {{ content }}

 <style>
/* Style the tab */
.tab {
  overflow: hidden;
  background-color: #ffffff;
}

/* Style the buttons inside the tab */
.tab button {
  background-color: inherit;
  float: left;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 10px 20px; /* Adjust padding to control the size of the tabs */
  transition: 0.3s;
  font-size: 17px;
  margin-right: 4px; /* Creates a small gap between tabs */
}

/* Change background color of buttons on hover */
.tab button:hover {
  background-color: #BED9FF;
}

/* Create an active/current tablink class */
.tab button.active {
  background-color: #BEE0FE;
}

/* Style the tab content */
.tabcontent {
  padding: 12px;
  border: none; /* No borders around the content */
}

 p.xsmall {
     line-height: 1.55;
     font-size: 9.5pt;
     margin-left: 40px;
 }

 p.small {
     line-height: 1.55;
     font-size: 11pt;
     margin-left: 40px;
 }

 ul.small {
     line-height: 1.55;
     font-size: 11pt;
 }

 p.small2 {
     line-height: 2.00;
     font-size: 11.5pt;
     margin-left: 40px;
 }

 p.medium {
     line-height: 1.55;
     font-size: 12.5pt;
     margin-left: 40px;
 }

 p.big {
     line-height: 1.55;
 }

 p.noindent {
     line-height: 1.55;
     font-size: 12pt;
 }

 .pub-entry {
     display: flex;
     align-items: flex-start;
     margin-bottom: 20px;
     gap: 20px;
 }

 .pub-image {
     flex-shrink: 0;
     width: 200px;
 }

 .pub-image img {
     width: 200px;
     height: auto;
     border-radius: 4px;
     box-shadow: 0 2px 6px rgba(0,0,0,0.1);
 }

 .pub-text {
     flex: 1;
     line-height: 1.55;
     font-size: 11pt;
 }

 </style>

 <p class="noindent">

    Hi! I'm a research scientist at <a href="https://ai.meta.com/research/" style="color: #57068C; text-decoration: none">Meta FAIR</a>, 
    where I work on building omni models that can understand and generate across many modalities (text, image, video, and more).
    I currently focus on post-training these models using advanced reward models and reinforcement learning.
    <br><br>
    
    I received my PhD from the University of Washington, 
    advised by Prof. <a href="https://people.ece.uw.edu/ostendorf/" style="color: #57068C; text-decoration: none">Mari Ostendorf</a> and Prof. <a href="https://nasmith.github.io/" style="color: #57068C; text-decoration: none">Noah A. Smith</a>, 
    and closely collaborated with Prof. <a href="http://www.ranjaykrishna.com/" style="color: #57068C; text-decoration: none">Ranjay Krishna</a>. 
    During my PhD, I was supported by the <a href="https://www.qualcomm.com/research/university-relations/innovation-fellowship/2025-north-america" style="color: #57068C; text-decoration: none">Qualcomm Innovation Fellowship</a> and Apple.  
    I have also interned at <a href="https://allenai.org/" style="color: #57068C; text-decoration: none">Allen Institute for AI (AI2)</a> and <a href="https://research.google/" style="color: #57068C; text-decoration: none">Google Research</a>.
    <br><br>

<!-- My research primarily focuses on building multimodal models that can understand, reason, and generate across many
modalities (text, image, video, ...).
I am also interested in building powerful multimodal agents with these models.
<br><br>
Prior to that, I graduated from the University of Chicago with B.S. in Mathematics, Computer Science, and Economics in
2021,
where I was fortunate to be advised by Prof. <a href="https://ttic.uchicago.edu/~klivescu/"
    style="color: #57068C; text-decoration: none">Karen Livescu</a> at <a href="https://www.ttic.edu/"
    style="color: #57068C; text-decoration: none">Toyota Technological Institute at Chicago (TTIC)</a>.
<br><br>
Before focusing on NLP and ML, I also helped applying computational methods in physics. I worked with
Dr. <a href="https://www.anl.gov/profile/saidur-rahman-bakaul" style="color: #57068C; text-decoration: none">Saidur Bakaul</a> at Argonne National Laboratory
and Prof. <a href="http://info.phys.tsinghua.edu.cn/henucl/ning.htm" style="color: #57068C; text-decoration: none">Chuangang Ning</a> at Tsinghua University. -->

<!-- <div style="border: 2px solid #e74747; 
            background-color: #fdeaea; 
            color: #e74747; 
            padding: 15px; 
            margin: 20px 0;
            font-size: 12pt;">
    I am on the job market this year!     <br>
    My research focuses on:
    <ul style="list-style-type: disc; margin-left: 20px;">
        <li>
            Better evaluations
            [<a href="https://arxiv.org/abs/2404.12390" style="color: #57068C; text-decoration: none;">BLINK(ECCV 2024)</a>,
            <a href="https://arxiv.org/abs/2303.11897" style="color: #57068C; text-decoration: none;">TIFA(ICCV 2023)</a>]
        </li>
        <li>
            Improving models with synthetic data
            [<a href="https://arxiv.org/abs/2312.03052" style="color: #57068C; text-decoration: none;">CVPR 2024
                oral</a>]
        </li>
        <li>
            RL with human/AI feedback
            [<a href="https://arxiv.org/abs/2306.01693" style="color: #57068C; text-decoration: none;">NeurIPS 2023
                spotlight</a>,
            <a href="https://arxiv.org/abs/2311.17946" style="color: #57068C; text-decoration: none;">NAACL 2025</a>]
        </li>
        <li>
            Multimodal agents
            [<a href="https://arxiv.org/abs/2406.09403" style="color: #57068C; text-decoration: none;">NeurIPS 2024</a>]
        </li>
        <li>Unified multimodal models [Llama-4]</li>
    </ul>
    Please contact me via email
    <a href="mailto:yushihu@uw.edu" style="color: #06478c; text-decoration: none;">yushihu@uw.edu</a>.

</div>
 -->









 <h3 style="font-weight: 500">Selected Publications</h3>
<span style="font-size: 10pt">Full list of publications on <a
        href="https://scholar.google.com/citations?user=mXN51X0AAAAJ&hl=en&oi=ao"
        style="color: #285095; text-decoration: none">Google Scholar</a></span>
<br>
<span style="font-size: 10pt">* indicates equal contribution</span>
 <hr>

    <div class="pub-entry">
        <div class="pub-image">
            <img src="images/MMRB.jpg" alt="MMRB2">
        </div>
        <div class="pub-text">
            <span style="font-weight:800"> <a href="https://github.com/facebookresearch/MMRB2"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image </a> <br> </span>
            <span style="line-height:170%"> <b>Yushi Hu*</b>, Reyhane Askari-Hemmat*, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad</span>
            <br>
            <span style="line-height:170%"> <i>Preprint 2025</i> </span>
            <br>
            [<a href="https://github.com/facebookresearch/MMRB2" style="color: #285095; text-decoration: none">code & data</a>]
            <br>
            <span style="color: #666; font-size: 10pt;"><b>TLDR:</b> A benchmark for reward models that advance SOTA omni models (e.g., Nano Banana).</span>
        </div>
    </div>


    <div class="pub-entry">
        <div class="pub-image">
            <img src="images/sketchpad.jpg" alt="Visual Sketchpad">
        </div>
        <div class="pub-text">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2406.09403"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models </a> <br> </span>
            <span style="line-height:170%"> <b>Yushi Hu*</b>, Weijia Shi*, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, Ranjay Krishna</span>
            <br>
            <span style="line-height:170%"> <i>NeurIPS 2024</i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2406.09403" style="color: #285095; text-decoration: none">paper</a>]
            [<a href="https://github.com/Yushi-Hu/VisualSketchpad" style="color: #285095; text-decoration: none">code</a>]
            [<a href="https://visualsketchpad.github.io/" style="color: #285095; text-decoration: none">project page</a>]
            <br>
            <span style="color: #666; font-size: 10pt;"><b>TLDR:</b> Proposes "thinking with images." Enables multimodal LLMs to generate images during reasoning, improving math, vision, and spatial reasoning tasks.</span>
        </div>
    </div>


    <div class="pub-entry">
        <div class="pub-image">
            <img src="images/blink.png" alt="BLINK">
        </div>
        <div class="pub-text">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2404.12390"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> BLINK: Multimodal Large Language Models Can See but Not Percieve </a> <br> </span>
            <span style="line-height:170%"> Xingyu Fu*, <b>Yushi Hu*</b>, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna</span>
            <br>
            <span style="line-height:170%"> <i>ECCV 2024</i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2404.12390" style="color: #285095; text-decoration: none">paper</a>]
            [<a href="https://zeyofu.github.io/blink/" style="color: #285095; text-decoration: none">project page</a>]
            [<a href="https://github.com/zeyofu/BLINK_Benchmark" style="color: #285095; text-decoration: none">code</a>]
            [<a href="https://huggingface.co/datasets/BLINK-Benchmark/BLINK" style="color: #285095; text-decoration: none">HF data</a>]
            <br>
            <span style="color: #666; font-size: 10pt;"><b>TLDR:</b> A benchmark revealing that multimodal LLMs struggle with core visual perception tasks that humans find trivial.</span>
        </div>
    </div>


    <div class="pub-entry">
        <div class="pub-image">
            <img src="images/visualprogramdistillation.jpg" alt="Visual Program Distillation">
        </div>
        <div class="pub-text">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2312.03052"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> Visual Program Distillation:
                    Distilling Tools and Programmatic Reasoning into Vision-Language Models </a> <br> </span>
            <span style="line-height:170%"> <b>Yushi Hu</b>, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata,
                Enming Luo, Ranjay Krishna, Ariel Fuxman</span>
            <br>
            <span style="line-height:170%"> <i>CVPR 2024 <span style="color: red">(Oral)</span></i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2312.03052" style="color: #285095; text-decoration: none">paper</a>]
            [<a href="https://visual-program-distillation.github.io/" style="color: #285095; text-decoration: none">project page</a>]
            <br>
            <span style="color: #666; font-size: 10pt;"><b>TLDR:</b> Distills tool usage and programmatic reasoning from visual programs into end-to-end vision-language models.</span>
        </div>
    </div>
    
    <div class="pub-entry">
        <div class="pub-image">
            <img src="images/finegrainedrlhf.png" alt="Fine-Grained RLHF">
        </div>
        <div class="pub-text">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2306.01693"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> Fine-Grained Human Feedback Gives Better
                    Rewards for Language Model Training </a> <br> </span>
            <span style="line-height:170%"> Zeqiu Wu*, <b>Yushi Hu*</b>, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj
                Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi</span>
            <br>
            <span style="line-height:170%"> <i>NeurIPS 2023 <span style="color: red">(Spotlight)</span></i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2306.01693" style="color: #285095; text-decoration: none">paper</a>]
            [<a href="https://FineGrainedRLHF.github.io/" style="color: #285095; text-decoration: none">project page</a>]
            [<a href="https://github.com/allenai/FineGrainedRLHF" style="color: #285095; text-decoration: none">code & data</a>]
            <br>
            <span style="color: #666; font-size: 10pt;"><b>TLDR:</b> Fine-grained feedback on sub-sentences enables better reward models and more effective RLHF training.</span>
        </div>
    </div>


    <div class="pub-entry">
        <div class="pub-image">
            <img src="images/tifa.png" alt="TIFA">
        </div>
        <div class="pub-text">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2303.11897"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> TIFA: Accurate and Interpretable
                    Text-to-Image Faithfulness Evaluation with Question Answering </a> <br> </span>
            <span style="line-height:170%"> <b>Yushi Hu</b>, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay
                Krishna, Noah A. Smith</span>
            <br>
            <span style="line-height:170%"> <i>ICCV 2023</i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2303.11897" style="color: #285095; text-decoration: none">paper</a>]
            [<a href="https://tifa-benchmark.github.io/" style="color: #285095; text-decoration: none">project page</a>]
            [<a href="https://github.com/Yushi-Hu/tifa" style="color: #285095; text-decoration: none">code & data</a>]
            [<a href="misc/TIFA_poster_new.pdf" style="color: #285095; text-decoration: none">poster</a>]
            <br>
            <span style="color: #666; font-size: 10pt;"><b>TLDR:</b> The first paper proposing to evaluate image generation with multimodal LLMs. </span>
        </div>
    </div>



 <!-- <h3 style="font-weight: 500">Presentations</h3>
 <hr>


 <ul class="small">

<li> Presentation on <i> Multilingual Jointly Trained Acoustic and Word Embeddings in EMNLP 2020 SIGTYP workshop (virtual) [<a href="https://slideslive.com/38939801/multilingual-jointly-trained-acoustic-and-written-word-embeddings" style="color: #285095; text-decoration: none">video</a>] </li>

 <li> Presentation on <i> Multilingual Jointly Trained Acoustic and Word Embeddings in INTERSPEECH 2020 (virtual) [<a href="http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=273&id=528" style="color: #285095; text-decoration: none">video</a>]</li>

 </ul> -->

 <h3 style="font-weight: 500">Experience</h3>
 <hr>

  <ul class="small">
<li>Research Scientist Meta FAIR, Seattle, WA, 2025</li>
<li>AI Research Scientist Intern, Meta GenAI, Menlo Park, CA, 2024</li>
<li> Student Researcher, Allen Institute for AI, Seattle, WA, 2024</li>
<li> Student Researcher, Google Research, Mountain View, CA, summer 2023</li>
 <li> Research Assistant, Toyota Technological Institute at Chicago (TTIC), Chicago, IL, 2019 - 2021</li>
</ul>

 <!-- <h3 style="font-weight: 500">Teaching</h3>
 <hr>

 <ul class="small">

 <li>Autumn 2018, Winter 2019. <i>Course Assistant</i>, MATH 20000: Mathematical Methods for Physical Science I<br></li>
 <li>Autumn 2019, Winter 2020. <i>Course Assistant</i>, MATH 20100: Mathematical Methods for Physical Science II<br></li>
 <li>Spring 2019. <i>Course Assistant</i>, MATH 20250: Abstract Linear Algebra<br></li>

 </ul> -->












 <!--
 <h3 style="font-weight: 500">Relevant Coursework</h3>
 <hr>

 <p class="xsmall">
 <span style="line-height:300%"> <u> At the University of Chicago (2015-2019) </u> </span>
 <br>
 CMSC 27230 - Honors Theory of Algorithms<br>
 CMSC 25025 / STAT 37601 - Machine Learning and Large-Scale Data Analysis (grad level, Lafferty)<br>
 CMSC 35400 / STAT 37710 - Machine Learning (grad level, Kondor)<br>
 TTIC 31020 - Statistical Machine Learning (grad level, Shakhnarovich)<br>
 TTIC 31190 - Natural Language Processing (grad level, Gimpel)<br>
 TTIC 41000 - Spectral Techniques (grad level, Stratos)<br>
 MATH 20300-20500 - Accelerated Real Analysis I, II, III<br>
 MATH 20250, 25400-25500 - Abstract Linear Algebra; Abstract Algebra I, II<br>
 BIOS 10602-10603 - Multiscale Modeling of Biological Systems I, II (computational biology)
  -->





 <br><br><br><br><br>
