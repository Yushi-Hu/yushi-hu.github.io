---
 layout: archive
---

 {{ content }}

 <html>
 <head>
 <meta name="google-site-verification" content="aAp-Ld0edambtgXw0_fF0tHhvll7gRpY4EOCXBkU7fM">
 <meta name="keywords" content="Yushi Hu, yushi hu, yushihu">
 <style>
/* Style the tab */
.tab {
  overflow: hidden;
  background-color: #ffffff;
}

/* Style the buttons inside the tab */
.tab button {
  background-color: inherit;
  float: left;
  border: none;
  outline: none;
  cursor: pointer;
  padding: 10px 20px; /* Adjust padding to control the size of the tabs */
  transition: 0.3s;
  font-size: 17px;
  margin-right: 4px; /* Creates a small gap between tabs */
}

/* Change background color of buttons on hover */
.tab button:hover {
  background-color: #BED9FF;
}

/* Create an active/current tablink class */
.tab button.active {
  background-color: #BEE0FE;
}

/* Style the tab content */
.tabcontent {
  padding: 12px;
  border: none; /* No borders around the content */
}

 p.xsmall {
     line-height: 1.55;
     font-size: 9.5pt;
     margin-left: 40px;
 }

 p.small {
     line-height: 1.55;
     font-size: 11pt;
     margin-left: 40px;
 }

 ul.small {
     line-height: 1.55;
     font-size: 11pt;
 }

 p.small2 {
     line-height: 2.00;
     font-size: 11.5pt;
     margin-left: 40px;
 }

 p.medium {
     line-height: 1.55;
     font-size: 12.5pt;
     margin-left: 40px;
 }

 p.big {
     line-height: 1.55;
 }

 p.noindent {
     line-height: 1.55;
     font-size: 12pt;
 }


 </style>
 </head>
 <body>



 <p class="noindent">

   Hi! I am a third-year PhD student in the Natural Language Processing (NLP) group at the University of Washington (UW), 
   advised by Prof. <a href="https://people.ece.uw.edu/ostendorf/" style="color: #57068C; text-decoration: none">Mari Ostendorf</a> and <a href="https://nasmith.github.io/" style="color: #57068C; text-decoration: none">Noah A. Smith</a>.
   I am also closely collaborating with Prof. <a href="http://www.ranjaykrishna.com/" style="color: #57068C; text-decoration: none">Ranjay Krishna</a>. 
   I work as a student researcher in <a href="https://allenai.org/" style="color: #57068C; text-decoration: none">Allen Institute for AI (AI2)</a>.
   Previously, I have also interned at <a href="https://research.google/" style="color: #57068C; text-decoration: none">Google Research</a> and Tencent AI.
<br><br>
   My research primary focuses on multimodal models (e.g., multimodal LLMs, image/3D/video generation models).
   My goal is to build models that are grounded in the multimodal world, capable of learning through interactions with humans, AIs, and the environment.
   I build evaluations that identify the critical weaknesses of existing models, and develop better supervision (e.g., better human/AI feedback, better self-training strategies) to train better models.
<br><br>
  Prior to that, I graduated from the University of Chicago with B.S. in Mathematics, Computer Science, and Economics in 2021,
  where I was fortunate to be advised by Prof. <a href="https://ttic.uchicago.edu/~klivescu/" style="color: #57068C; text-decoration: none">Karen Livescu</a> at <a href="https://www.ttic.edu/" style="color: #57068C; text-decoration: none">Toyota Technological Institute at Chicago (TTIC)</a>.
 <!-- <br><br>
Before focusing on NLP and ML, I also helped applying computational methods in physics. I worked with
Dr. <a href="https://www.anl.gov/profile/saidur-rahman-bakaul" style="color: #57068C; text-decoration: none">Saidur Bakaul</a> at Argonne National Laboratory
and Prof. <a href="http://info.phys.tsinghua.edu.cn/henucl/ning.htm" style="color: #57068C; text-decoration: none">Chuangang Ning</a> at Tsinghua University. -->

 <p>






 <h3 style="font-weight: 500">Publications</h3>
<span style="font-size: 10pt">Most recent publications on <a href="https://www.semanticscholar.org/author/Yushi-Hu/2112209725"
    style="color: #285095; text-decoration: none">Semantic Scholar</a> and <a
        href="https://scholar.google.com/citations?user=mXN51X0AAAAJ&hl=en&oi=ao"
        style="color: #285095; text-decoration: none">Google Scholar</a></span>
<br>
<span style="font-size: 10pt">* indicates equal contribution</span>
 <hr>



<!-- Tab links -->
<div class="tab">
    <button class="tablinks"  id="defaultOpen" onclick="openTab(event, 'Selected')">Selected Publications</button>
    <button class="tablinks" onclick="openTab(event, 'All')">All Publications</button>
</div>

<!-- Tab content for Selected Publications -->
<div id="Selected" class="tabcontent">

    <h4>Better Evaluations for Multimodal Generative Models</h4>

    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2404.12390"
                style="font-size: 12pt; color: #285095; text-decoration: none"> BLINK: Multimodal Large Language Models Can See but Not Percieve </a> <br> </span>
        <span style="line-height:170%"> Xingyu Fu*, <b>Yushi Hu*</b>, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna</span>
        <br>
        <span style="line-height:170%"> <i>preprint</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2404.12390" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://zeyofu.github.io/blink/" style="color: #285095; text-decoration: none">project page</a>]
        [<a href="https://github.com/zeyofu/BLINK_Benchmark" style="color: #285095; text-decoration: none">code</a>]
        [<a href="https://huggingface.co/datasets/BLINK-Benchmark/BLINK" style="color: #285095; text-decoration: none">HF data</a>]


    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2303.11897"
                style="font-size: 12pt; color: #285095; text-decoration: none"> TIFA: Accurate and Interpretable
                Text-to-Image Faithfulness Evaluation with Question Answering </a> <br> </span>
        <span style="line-height:170%"> <b>Yushi Hu</b>, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay
            Krishna, Noah A. Smith</span>
        <br>
        <span style="line-height:170%"> <i>ICCV 2023</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2303.11897" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://tifa-benchmark.github.io/" style="color: #285095; text-decoration: none">project page</a>]
        [<a href="https://github.com/Yushi-Hu/tifa" style="color: #285095; text-decoration: none">code & data</a>]
        [<a href="../misc/TIFA_poster_new.pdf" style="color: #285095; text-decoration: none">poster</a>]

    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2310.18235"
                style="font-size: 12pt; color: #285095; text-decoration: none"> Davidsonian Scene Graph: Improving
                Reliability in Fine-Grained Evaluation for Text-Image Generation </a> <br> </span>
        <span style="line-height:170%"> Jaemin Cho, <b>Yushi Hu</b>, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason
            Baldridge, Mohit Bansal, Jordi Pont-Tuset, Su Wang</span>
        <br>
        <span style="line-height:170%"> <i>ICLR 2024</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2310.18235" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://google.github.io/dsg/" style="color: #285095; text-decoration: none">project page</a>]
        [<a href="https://github.com/j-min/DSG" style="color: #285095; text-decoration: none">code & data</a>]


    <h4>Improving Models with Better Supervision</h4>

        <p class="small">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2312.03052"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> Visual Program Distillation:
                    Distilling Tools and Programmatic Reasoning into Vision-Language Models </a> <br> </span>
            <span style="line-height:170%"> <b>Yushi Hu</b>, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata,
                Enming Luo, Ranjay Krishna, Ariel Fuxman</span>
            <br>
            <span style="line-height:170%"> <i>CVPR 2024 <span style="color: red">(Oral)</span></i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2312.03052" style="color: #285095; text-decoration: none">paper</a>]
            [<a href="https://visual-program-distillation.github.io/" style="color: #285095; text-decoration: none">project
                page</a>]
    
        <p class="small">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2306.01693"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> Fine-Grained Human Feedback Gives Better
                    Rewards for Language Model Training </a> <br> </span>
            <span style="line-height:170%"> Zeqiu Wu*, <b>Yushi Hu*</b>, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj
                Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi</span>
            <br>
            <span style="line-height:170%"> <i>NeurIPS 2023 <span style="color: red">(Spotlight)</span></i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2306.01693" style="color: #285095; text-decoration: none">paper</a>]
            [<a href="https://FineGrainedRLHF.github.io/" style="color: #285095; text-decoration: none">project page</a>]
            [<a href="https://github.com/allenai/FineGrainedRLHF" style="color: #285095; text-decoration: none">code & data</a>]


    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2311.17946"
                style="font-size: 12pt; color: #285095; text-decoration: none"> DreamSync: Aligning Text-to-Image Generation
                with Image Understanding Feedback </a> <br> </span>
        <span style="line-height:170%"> Jiao Sun*, Deqing Fu*, <b>Yushi Hu*</b>, Su Wang, Royi Rassin, Da-Cheng Juan, Dana
            Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus
            Rashtchian</span>
        <br>
        <span style="line-height:170%"> <i>Preprint 2023</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2311.17946" style="color: #285095; text-decoration: none">paper</a>]
    
    

</div>

<!-- Tab content for All Publications -->
<div id="All" class="tabcontent">

        <p class="small">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2404.1239"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> BLINK: Multimodal Large Language Models Can
                    See but Not Percieve </a> <br> </span>
            <span style="line-height:170%"> Xingyu Fu*, <b>Yushi Hu*</b>, Bangzheng Li, Yu Feng, Haoyu Wang, Xudong Lin, Dan
                Roth, Noah A. Smith, Wei-Chiu Ma, Ranjay Krishna</span>
            <br>
            <span style="line-height:170%"> <i>preprint</i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2404.12390" style="color: #285095; text-decoration: none">paper</a>]
            [<a href="https://zeyofu.github.io/blink/" style="color: #285095; text-decoration: none">project page</a>]
            [<a href="https://github.com/zeyofu/BLINK_Benchmark" style="color: #285095; text-decoration: none">code</a>]
            [<a href="https://huggingface.co/datasets/BLINK-Benchmark/BLINK" style="color: #285095; text-decoration: none">HF
                data</a>]

            <br>
            <span style="font-size: 10pt">TLDR: We introduce Blink, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception
            abilities not found in other evaluations.
            </span>

        <p class="small">
            <span style="font-weight:800"> <a href="https://arxiv.org/abs/2402.04315"
                    style="font-size: 12pt; color: #285095; text-decoration: none"> Training Language Models to Generate Text with Citations via Fine-grained Rewards </a> <br> </span>
            <span style="line-height:170%"> Chengyu Huang, Zeqiu Wu, <b>Yushi Hu</b>, Wenya Wang</span>
            <br>
            <span style="line-height:170%"> <i>preprint</i> </span>
            <br>
            [<a href="https://arxiv.org/abs/2402.04315" style="color: #285095; text-decoration: none">paper</a>]
        
            <br>
            <span style="font-size: 10pt">TLDR: Using fine-grained rewards to train LLMs to generate text with citations.
            </span>



    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2312.03052"
                style="font-size: 12pt; color: #285095; text-decoration: none"> Visual Program Distillation:
                Distilling Tools and Programmatic Reasoning into Vision-Language Models </a> <br> </span>
        <span style="line-height:170%"> <b>Yushi Hu</b>, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata,
            Enming Luo, Ranjay Krishna, Ariel Fuxman</span>
        <br>
        <span style="line-height:170%"> <i>CVPR 2024 <span style="color: red">(Oral)</span></i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2312.03052" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://visual-program-distillation.github.io/" style="color: #285095; text-decoration: none">project
            page</a>]
    
        <br>
        <span style="font-size: 10pt">TLDR: Using LLM-generated codes + vision tools to generate high-quality multimodal
            chain-of-thought reasoning data for large multimodal model (LMM) training.
            The resulting models, PaLI-3-VPD (5B) and PaLI-X-VPD (55B) set the new SOTA for many existing vision-Language
            tasks.
        </span>
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2311.17946"
                style="font-size: 12pt; color: #285095; text-decoration: none"> DreamSync: Aligning Text-to-Image Generation
                with Image Understanding Feedback </a> <br> </span>
        <span style="line-height:170%"> Jiao Sun*, Deqing Fu*, <b>Yushi Hu*</b>, Su Wang, Royi Rassin, Da-Cheng Juan, Dana
            Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus
            Rashtchian</span>
        <br>
        <span style="line-height:170%"> <i>Preprint 2023</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2311.17946" style="color: #285095; text-decoration: none">paper</a>]
    
        <br>
        <span style="font-size: 10pt">TLDR: Using TIFA as the reward model for text-to-image generation models. Improve the
            text-to-image faithfulness and image aesthetics with simple rejection-sampling fine-tuning.</span>
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2310.18235"
                style="font-size: 12pt; color: #285095; text-decoration: none"> Davidsonian Scene Graph: Improving
                Reliability in Fine-Grained Evaluation for Text-Image Generation </a> <br> </span>
        <span style="line-height:170%"> Jaemin Cho, <b>Yushi Hu</b>, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason
            Baldridge, Mohit Bansal, Jordi Pont-Tuset, Su Wang</span>
        <br>
        <span style="line-height:170%"> <i>ICLR 2024</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2310.18235" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://google.github.io/dsg/" style="color: #285095; text-decoration: none">project page</a>]
        [<a href="https://github.com/j-min/DSG" style="color: #285095; text-decoration: none">code & data</a>]
        <br>
        <span style="font-size: 10pt">TLDR: An improved and more reliable version of TIFA for text-to-image evaluation,
            based on Davidsonian semantics and scene graphs.</span>
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2306.01693"
                style="font-size: 12pt; color: #285095; text-decoration: none"> Fine-Grained Human Feedback Gives Better
                Rewards for Language Model Training </a> <br> </span>
        <span style="line-height:170%"> Zeqiu Wu*, <b>Yushi Hu*</b>, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj
            Ammanabrolu, Noah A. Smith, Mari Ostendorf, Hannaneh Hajishirzi</span>
        <br>
        <span style="line-height:170%"> <i>NeurIPS 2023 <span style="color: red">(Spotlight)</span></i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2306.01693" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://FineGrainedRLHF.github.io/" style="color: #285095; text-decoration: none">project page</a>]
        [<a href="https://github.com/allenai/FineGrainedRLHF" style="color: #285095; text-decoration: none">code & data</a>]
        <br>
        <span style="font-size: 10pt">TLDR: F in current RLHF is overall preference, which conveys limited information. We
            introduce Fine-Grained RLHF and train LMs with explicit feedback like "sentence 1 is not factual", "sentence 2
            is
            toxic". We show that Fine-Grained RLHF is more effective and enables customizing LMs for specific needs.</span>
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2303.11897"
                style="font-size: 12pt; color: #285095; text-decoration: none"> TIFA: Accurate and Interpretable
                Text-to-Image Faithfulness Evaluation with Question Answering </a> <br> </span>
        <span style="line-height:170%"> <b>Yushi Hu</b>, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay
            Krishna, Noah A. Smith</span>
        <br>
        <span style="line-height:170%"> <i>ICCV 2023</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2303.11897" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://tifa-benchmark.github.io/" style="color: #285095; text-decoration: none">project page</a>]
        [<a href="https://github.com/Yushi-Hu/tifa" style="color: #285095; text-decoration: none">code & data</a>]
        [<a href="../misc/TIFA_poster_new.pdf" style="color: #285095; text-decoration: none">poster</a>]
        <br>
        <span style="font-size: 10pt">TLDR: Fine-grained and accurate evaluation of synthesized images using Image-to-Text
            Models (e.g. GPT-4, BLIP-2, etc.) and Large Language Models (e.g. GPT-3.5). More accurate than CLIP!</span>
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2211.09699"
                style="font-size: 12pt; color: #285095; text-decoration: none"> PromptCap: Prompt-Guided Task-Aware Image
                Captioning </a> <br> </span>
        <span style="line-height:170%"> <b>Yushi Hu*</b>, Hang Hua*, Zhengyuan Yang, Weijia Shi, Noah A. Smith, Jiebo
            Luo</span>
        <br>
        <span style="line-height:170%"> <i>ICCV 2023</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2211.09699" style="color: #285095; text-decoration: none">paper</a>][<a
            href="https://yushi-hu.github.io/promptcap_demo/" style="color: #285095; text-decoration: none">project
            page</a>]
        [<a href="https://huggingface.co/tifa-benchmark/promptcap-coco-vqa"
            style="color: #285095; text-decoration: none">Huggingface Checkpoint</a>]
        [<a href="../misc/Promptcap poster.pdf" style="color: #285095; text-decoration: none">poster</a>]
        <br>
        <span style="font-size: 10pt">TLDR: A captioning model that is controlled by natural language instruction. Simple
            and effective visual frontend for LLMs like GPT-3 and ChatGPT.</span>
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2212.09741"
                style="font-size: 12pt; color: #285095; text-decoration: none"> One Embedder, Any Task:
                Instruction-Finetuned Text Embeddings </a> <br> </span>
        <span style="line-height:170%"> Hongjin Su*, Weijia Shi*, Jungo Kasai, Yizhong Wang, <b>Yushi Hu</b>, Mari
            Ostendorf, Wen-tau Yih, Noah A. Smith, Luke
            Zettlemoyer, Tao Yu</span>
        <br>
        <span style="line-height:170%"> <i>ACL 2023</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2212.09741" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://instructor-embedding.github.io/" style="color: #285095; text-decoration: none">project page</a>]
        [<a href="https://huggingface.co/hkunlp/instructor-large"
            style="color: #285095; text-decoration: none">checkpoint</a>]
        <br>
        <span style="font-size: 10pt">TLDR: Instruction-finetuned text embeddings. The SOTA embedding for retrieval,
            semantic similarity, etc. Open-source, and better than OpenAI embeddings!</span>
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2210.02875"
                style="font-size: 12pt; color: #285095; text-decoration: none"> Binding Language Models in Symbolic
                Languages </a> <br> </span>
        <span style="line-height:170%"> Zhoujun Cheng*, Tianbao Xie*, Peng Shi, Chengzu Li, Rahul Nadkarni, <b>Yushi Hu</b>,
            Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, Tao Yu</span>
        <br>
        <span style="line-height:170%"> <i>ICLR 2023 <span style="color: red">(Spotlight)</span></i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2210.02875" style="color: #285095; text-decoration: none">paper</a>][<a
            href="https://lm-code-binder.github.io/" style="color: #285095; text-decoration: none">project page</a>]
        <br>
        <span style="font-size: 10pt">TLDR: Combining GPT-3 with Python and SQL. Proposes the concept of Toolformer and
            ChatGPT plugins.</span>
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2205.12244"
                style="font-size: 12pt; color: #285095; text-decoration: none"> Unsupervised Learning of Hierarchical
                Conversation Structure </a> <br> </span>
        <span style="line-height:170%"> Bo-Ru Lu, <b>Yushi Hu</b>, Hao Cheng, Noah A. Smith, Mari Ostendorf</span>
        <br>
        <span style="line-height:170%"> <i>EMNLP 2022</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2205.12244" style="color: #285095; text-decoration: none">paper</a>]
        <br>
        <span style="font-size: 10pt">TLDR: Learning the common dialogue structure from a huge amount of customer-service
            dialogues.</span>
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2203.08568"
                style="font-size: 12pt; color: #285095; text-decoration: none"> In-Context Learning for Few-Shot Dialogue
                State Tracking </a> <br> </span>
        <span style="line-height:170%"> <b>Yushi Hu</b>, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith, Mari
            Ostendorf</span>
        <br>
        <span style="line-height:170%"> <i>EMNLP 2022</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2203.08568" style="color: #285095; text-decoration: none">paper</a>] [<a
            href="https://github.com/Yushi-Hu/IC-DST" style="color: #285095; text-decoration: none">code</a>] [<a
            href="../misc/hu2022context.txt" style="color: #285095; text-decoration: none">bibtex</a>]
        <span style="font-size: 10pt">
            <br>
            TLDR: The first paper that shows GPT-3 is surprisingly good at dialogue understanding tasks.</span>
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2011.11807"
                style="font-size: 12pt; color: #285095; text-decoration: none"> Acoustic Span Embeddings for Multilingual
                Query-by-Example Search </a> <br> </span>
        <span style="line-height:170%"> <b>Yushi Hu</b>, Shane Settle, Karen Livescu</span>
        <br>
        <span style="line-height:170%"> <i>IEEE Spoken Language Technology Workshop (SLT 2021)</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2011.11807" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://github.com/Yushi-Hu/Query-by-Example" style="color: #285095; text-decoration: none">code</a>]
        [<a href="../misc/hu2020acoustic.txt" style="color: #285095; text-decoration: none">bibtex</a>]
        [<a href="../misc/huslt2021presentation.pdf" style="color: #285095; text-decoration: none">slides</a>]
    
    
    <p class="small">
        <span style="font-weight:800"> <a href="https://arxiv.org/abs/2006.14007"
                style="font-size: 12pt; color: #285095; text-decoration: none"> Multilingual Jointly Trained Acoustic and
                Written Word Embeddings </a> <br> </span>
        <span style="line-height:170%"> <b>Yushi Hu</b>, Shane Settle, Karen Livescu</span>
        <br>
        <span style="line-height:170%"> <i>InterSpeech 2020</i> </span>
        <br>
        [<a href="https://arxiv.org/abs/2006.14007" style="color: #285095; text-decoration: none">paper</a>]
        [<a href="https://github.com/Yushi-Hu/Multilingual-AWE" style="color: #285095; text-decoration: none">code</a>]
        [<a href="../misc/hu2020multilingual.txt" style="color: #285095; text-decoration: none">bibtex</a>]
        [<a href="../misc/hu2020presentation.pdf" style="color: #285095; text-decoration: none">slides</a>]
        [<a href="http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=273&id=528"
            style="color: #285095; text-decoration: none">video</a>]
        [<a href="http://projector.tensorflow.org/?config=https://raw.githubusercontent.com/Yushi-Hu/Multilingual-AWE/master/emb-examples/awe-agwe-config.json"
            style="color: #285095; text-decoration: none">demo</a>]
    
    
    <p class="small">
        <span style="font-weight:800"> <a
                href="https://www.researchgate.net/profile/Sergei-Prokhorenko/publication/354697530_Freestanding_Ferroelectric_Bubble_Domains/links/6149d315a595d06017df3f5b/Freestanding-Ferroelectric-Bubble-Domains.pdf"
                style="font-size: 12pt; color: #285095; text-decoration: none">Freestanding Ferroelectric Bubble Domains</a>
            <br> </span>
        <span style="line-height:170%"> Saidur R Bakaul, Sergei Prokhorenko, Qi Zhang, Yousra Nahas, <b>Yushi Hu</b>, Amanda
            Petford-Long, Laurent Bellaiche, Nagarajan
            Valanoor</span>
        <br>
        <span style="line-height:170%"> <i>Advanced Materials, 2021</i> </span>
        <br>
        [<a href="https://www.researchgate.net/profile/Sergei-Prokhorenko/publication/354697530_Freestanding_Ferroelectric_Bubble_Domains/links/6149d315a595d06017df3f5b/Freestanding-Ferroelectric-Bubble-Domains.pdf"
            style="color: #285095; text-decoration: none">paper</a>]
    
        <!-- <p class="small">
     <span style="font-weight:800"> <a href="https://arxiv.org/abs/1608.06936" style="font-size: 12pt; color: #285095; text-decoration: none"> A Simple Setup to Measure Muon Lifetime and Electron Energy Spectrum of Muon Decay and its Monte Carlo Simulation</a> <br> </span>
     <span style="line-height:170%"> <b>Yushi Hu</b>, Tianye Wang, Yefeng Mei, Zhao Zhang, Chuangang Ning</span>
     <br>
     <span style="line-height:170%"> <i>物理与工程, 2016.05</i> </span>
     <br>
    [<a href="https://arxiv.org/pdf/1608.06936" style="color: #285095; text-decoration: none">English version preprint</a>] -->

</div>

<script>
    function openTab(evt, tabName) {
        var i, tabcontent, tablinks;
        tabcontent = document.getElementsByClassName("tabcontent");
        for (i = 0; i < tabcontent.length; i++) {
            tabcontent[i].style.display = "none";
        }
        tablinks = document.getElementsByClassName("tablinks");
        for (i = 0; i < tablinks.length; i++) {
            tablinks[i].className = tablinks[i].className.replace(" active", "");
        }
        document.getElementById(tabName).style.display = "block";
        evt.currentTarget.className += " active";
    }

    // Get the element with id="defaultOpen" and click on it
    document.getElementById("defaultOpen").click();
</script>




 <!-- <h3 style="font-weight: 500">Presentations</h3>
 <hr>


 <ul class="small">

<li> Presentation on <i> Multilingual Jointly Trained Acoustic and Word Embeddings in EMNLP 2020 SIGTYP workshop (virtual) [<a href="https://slideslive.com/38939801/multilingual-jointly-trained-acoustic-and-written-word-embeddings" style="color: #285095; text-decoration: none">video</a>] </li>

 <li> Presentation on <i> Multilingual Jointly Trained Acoustic and Word Embeddings in INTERSPEECH 2020 (virtual) [<a href="http://www.interspeech2020.org/index.php?m=content&c=index&a=show&catid=273&id=528" style="color: #285095; text-decoration: none">video</a>]</li>

 </ul> -->

 <h3 style="font-weight: 500">Work Experience</h3>
 <hr>

  <ul class="small">
<li> Student Researcher, Allen Institute for AI, Seattle, WA, 2024</li>
<li> Student Researcher, Google Research, Mountain View, CA, summer 2023</li>
<li> Research Intern, Tencent AI Lab (Seattle), Bellevue, WA, summer 2022</li>
 <li> Research Assistant, Toyota Technological Institute at Chicago (TTIC), Chicago, IL, 2019 - 2021</li>
 <li> Research Intern, Argonne National Laboratory, Lemont, IL, summer 2018</li>
</ul>

 <!-- <h3 style="font-weight: 500">Teaching</h3>
 <hr>

 <ul class="small">

 <li>Autumn 2018, Winter 2019. <i>Course Assistant</i>, MATH 20000: Mathematical Methods for Physical Science I<br></li>
 <li>Autumn 2019, Winter 2020. <i>Course Assistant</i>, MATH 20100: Mathematical Methods for Physical Science II<br></li>
 <li>Spring 2019. <i>Course Assistant</i>, MATH 20250: Abstract Linear Algebra<br></li>

 </ul> -->












 <!--
 <h3 style="font-weight: 500">Relevant Coursework</h3>
 <hr>

 <p class="xsmall">
 <span style="line-height:300%"> <u> At the University of Chicago (2015-2019) </u> </span>
 <br>
 CMSC 27230 - Honors Theory of Algorithms<br>
 CMSC 25025 / STAT 37601 - Machine Learning and Large-Scale Data Analysis (grad level, Lafferty)<br>
 CMSC 35400 / STAT 37710 - Machine Learning (grad level, Kondor)<br>
 TTIC 31020 - Statistical Machine Learning (grad level, Shakhnarovich)<br>
 TTIC 31190 - Natural Language Processing (grad level, Gimpel)<br>
 TTIC 41000 - Spectral Techniques (grad level, Stratos)<br>
 MATH 20300-20500 - Accelerated Real Analysis I, II, III<br>
 MATH 20250, 25400-25500 - Abstract Linear Algebra; Abstract Algebra I, II<br>
 BIOS 10602-10603 - Multiscale Modeling of Biological Systems I, II (computational biology)
  -->





 <br><br><br><br><br>
